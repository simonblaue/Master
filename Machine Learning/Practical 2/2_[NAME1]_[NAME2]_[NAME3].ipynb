{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Machine Learning - Practical 2\n",
        "\n",
        "Names: {YOUR NAMES}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 1: The Data\n",
        "\n",
        "We use the mouse protein expression dataset: https://archive.ics.uci.edu/ml/datasets/Mice+Protein+Expression\n",
        "Please use the code provided below for loading the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "file = 'Data_Cortex_Nuclear.csv'\n",
        "df = pd.read_csv(file)\n",
        "\n",
        "N = 10  # use only every tenth sample\n",
        "X_all = df.iloc[::N,1:65].to_numpy()\n",
        "t_all = (df['Behavior'] == 'S/C').to_numpy()[::N]\n",
        "\n",
        "idx = ~np.any(np.isnan(X_all), axis=1)\n",
        "X_all = X_all[idx]\n",
        "t_all = t_all[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tasks 1.1: Dataset Exploration \n",
        "\n",
        "Let's start with a bit of exploration.\n",
        "\n",
        "- How many samples / features are provided?\n",
        "- How many labels does the dataset have?\n",
        "- What is the value range of the individuals predictors?\n",
        "- Visualize the 10 first samples of the dataset in a form that highlights their differences.\n",
        "- Visualize the variance of each predictor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of (samples,features):  (105, 64)\n",
            "Number of targets:  (105,)\n",
            "Number of labels:  82\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# data exploration\n",
        "print(\"Number of (samples,features): \", X_all.shape)\n",
        "print( \"Number of targets: \", t_all.shape)\n",
        "print(\"Number of labels: \", len(df.columns))\n",
        "# for row in df.iloc[::N,1:65].to_numpy():\n",
        "#     print(min(row), max(row))\n",
        "df.head()\n",
        "\n",
        "for i in range(5):  \n",
        "    plt.plot(X_all[:,i], X_all[:,i], marker='.', linestyle='')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Task 1.2: Data Preprocessing**:  \n",
        "- Write a function `split_data(X, y, frac, seed)` that first shuffles your training data and then splits it into a training and a test set. `frac` determines the relative size of the test dataset, `seed` makes sure we get reproducible \n",
        "results.  \n",
        "- Write a function `preprocess(X)` which zero-centers your data and sets variance to one (per-feature)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_data(X, y, frac=0.3, seed=None):\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    # ---------------- INSERT CODE ----------------------\n",
        "    idx = X.shape[0]\n",
        "\n",
        "    n_trainset = int(idx*(1-frac))  # size of the training set\n",
        "    n_testset = idx-n_trainset  # size of the test set\n",
        "\n",
        "    idx_shuffled = np.random.permutation(idx)\n",
        "\n",
        "    test_idx = idx_shuffled[:n_testset]\n",
        "    train_idx = idx_shuffled[n_testset:n_testset+n_trainset]\n",
        "\n",
        "    X_test = X[test_idx]\n",
        "    y_test = y[test_idx]\n",
        "    print('Test set shapes (X and y)', X_test.shape, y_test.shape)\n",
        "\n",
        "    X_train = X[train_idx]\n",
        "    y_train = y[train_idx]\n",
        "    print('Training set shapes (X and y):', X_train.shape, y_train.shape)\n",
        "    \n",
        "\n",
        "    # ---------------- END CODE -------------------------\n",
        "    \n",
        "    return X_train, X_test, y_train, y_test\n",
        "    \n",
        "    \n",
        "def preprocess(X, verbose=False):\n",
        "    # if seed is not None:\n",
        "    #     np.random.seed(seed)\n",
        "\n",
        "    # ---------------- INSERT CODE ----------------------\n",
        "    mean_old = np.mean(X, axis=1)\n",
        "    if verbose: print(\"Means: \", mean_old)\n",
        "    X = (X.transpose()-mean_old).transpose()\n",
        "\n",
        "    variance_old = np.var(X, axis=1)\n",
        "    if verbose: print(\"Variances: \", variance_old)\n",
        "    X = (X.transpose()/variance_old).transpose()\n",
        "\n",
        "    \n",
        "    # ---------------- END CODE -------------------------\n",
        "    \n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Means:  [0.82457944 0.63690084 0.69770272 0.88018217 0.80147942 0.86277105\n",
            " 0.74850663 0.65689898 0.81241391 0.79380013 0.60432627 0.72754584\n",
            " 0.88976284 0.73587585 0.82739393 0.77793991 0.73792629 0.76326964\n",
            " 0.77193051 0.56553929 0.64441075 0.71292868 0.73322197 0.7534914\n",
            " 0.77786411 0.64796155 0.64858381 0.77780769 0.7443122  0.72577247\n",
            " 0.87776855 0.78251255 0.8325154  0.72893044 1.05356017 0.84172741\n",
            " 0.84962168 0.8965331  0.63294406 0.75760884 0.74193602 0.63937542\n",
            " 0.75589787 0.88254102 0.71435637 0.75126766 0.79605345 0.69690483\n",
            " 0.68089778 0.78509797 0.67548589 0.79513098 0.67100753 0.5992903\n",
            " 0.76062392 0.6612079  0.70384329 0.61914814 0.56024019 0.64896265\n",
            " 0.74969071 0.58989776 0.77509964 0.92770347 0.74095563 0.84203632\n",
            " 0.85964807 0.77649164 0.74192806 0.86572663 0.68631325 0.69579796\n",
            " 0.87422275 0.77276198 0.85955193 0.83350357 0.73077057 0.80832487\n",
            " 0.8117968  0.74501336 0.75743868 0.73242588 0.64045018 0.78962156\n",
            " 0.76928567 0.56563876 0.7195436  0.78839706 0.55875882 0.75590449\n",
            " 0.83633111 0.68513691 0.68285838 0.76304502 0.54720998 0.795864\n",
            " 0.88582957 0.7699173  0.77431188 0.9340572  0.73111543 0.96920481\n",
            " 0.7300027  0.6542807  0.757055  ]\n",
            "Variances:  [1.04407625 0.4476786  0.61734581 1.11215015 0.80900073 1.12057989\n",
            " 0.82029872 0.52546139 0.87101167 0.82779477 0.42900463 0.71952325\n",
            " 0.97969122 0.5904079  0.95681061 0.99889225 0.75391382 0.95405745\n",
            " 1.00025853 0.45037763 0.56205649 0.79929924 0.6403141  0.9237875\n",
            " 1.0255562  0.52609765 0.54984646 0.9446187  0.7531568  0.72305244\n",
            " 1.0532603  0.71694696 0.9267627  0.79353379 1.53402721 0.90757102\n",
            " 1.14680866 0.95745495 0.47985814 0.73669543 0.80587535 0.5075469\n",
            " 0.73375219 1.20903393 0.66941416 0.71505748 0.89254894 0.56366865\n",
            " 0.56985624 0.84675972 0.55611631 0.91798899 0.59784881 0.3516111\n",
            " 0.70494104 0.48707959 0.58686922 0.43471899 0.29680129 0.43767081\n",
            " 0.64644437 0.33668776 0.70044541 1.3928298  0.90497413 1.00874835\n",
            " 1.03722065 0.76699138 0.68284376 1.08830248 0.5156106  0.5161352\n",
            " 1.18171981 0.71676834 0.9878486  1.27558411 0.80803272 1.01128468\n",
            " 1.06724282 0.69499781 0.85252449 0.63454547 0.39751879 0.7762154\n",
            " 0.73960633 0.33551465 0.58455415 0.79655043 0.33257362 0.62849996\n",
            " 0.87120134 0.58500774 0.49145016 0.70461772 0.29192659 0.72540493\n",
            " 1.1171949  0.84576868 0.91178484 1.60117635 0.69042921 1.61588036\n",
            " 0.63409794 0.45616769 0.67365579]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([0.95778446, 2.23374538, 1.61983768, 0.89915917, 1.23609282,\n",
              "       0.8923951 , 1.2190681 , 1.90308939, 1.14809025, 1.20802889,\n",
              "       2.33097718, 1.38980916, 1.02072978, 1.69374428, 1.04513891,\n",
              "       1.00110897, 1.32641155, 1.04815491, 0.99974153, 2.22035896,\n",
              "       1.7791806 , 1.2510959 , 1.56173353, 1.08250003, 0.97508064,\n",
              "       1.90078781, 1.81868952, 1.0586282 , 1.32774477, 1.38302556,\n",
              "       0.94943292, 1.39480332, 1.07902487, 1.26018578, 0.65187892,\n",
              "       1.10184215, 0.87198504, 1.04443557, 2.08394921, 1.35741306,\n",
              "       1.24088669, 1.97026126, 1.36285793, 0.82710665, 1.49384351,\n",
              "       1.39848897, 1.12038674, 1.77409192, 1.75482856, 1.18097257,\n",
              "       1.79818498, 1.08933768, 1.67266369, 2.84405124, 1.41855836,\n",
              "       2.05305257, 1.70395715, 2.30033661, 3.36925754, 2.28482226,\n",
              "       1.54692352, 2.97011096, 1.427663  , 0.71796281, 1.10500396,\n",
              "       0.99132752, 0.96411501, 1.30379562, 1.46446385, 0.91886219,\n",
              "       1.93944812, 1.93747684, 0.84622428, 1.3951509 , 1.01230087,\n",
              "       0.78395458, 1.23757365, 0.98884124, 0.93699389, 1.43885346,\n",
              "       1.17298683, 1.57593119, 2.51560436, 1.28830218, 1.35207063,\n",
              "       2.98049579, 1.7107055 , 1.2554133 , 3.00685303, 1.59109   ,\n",
              "       1.14784029, 1.70937908, 2.03479434, 1.41920926, 3.4255187 ,\n",
              "       1.3785404 , 0.89509897, 1.18235639, 1.09674997, 0.62454083,\n",
              "       1.44837441, 0.6188577 , 1.57704345, 2.19217632, 1.48443762])"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_temp = preprocess(X_all, verbose=True)\n",
        "\n",
        "np.var(x_temp, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 2: LDA\n",
        "\n",
        "First, use Linear Discriminant Analysis to separate the classes. As discussed in the Bishop in pg. 186-189, we can find a weight vector $\\vec{w}$ that performs a projection of the i-th input data point $\\vec{x}_i$\n",
        "\n",
        "$p =   \\vec{w}^T \\vec{x}_i$\n",
        "\n",
        "that optimally separates the classes.\n",
        "\n",
        "Use the analytic solution to compute the optimal weights $\\vec{w}$ from the training set data. \n",
        "\n",
        "### Task 2.1\n",
        "\n",
        "1. Implement a function `compute_lda_weights(x, y)` manually, which carries out LDA using the data `x,y`. \n",
        "2. Apply this function on your training data.\n",
        "3. Visualize the obtained weight vector $\\vec{w}$ using a `plt.stemplot`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# example usage of stemplot\n",
        "w_lda = np.random.uniform(-1, 1, 50)  # example data\n",
        "plt.stem(w_lda.flatten(), use_line_collection=True)\n",
        "plt.title('Computed LDA weights')\n",
        "plt.ylabel('weight')\n",
        "plt.xlabel('predictors')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_lda_weights(x, y, alpha=0.001):\n",
        "    '''function that computes regularized LDA weights for a two class problem\n",
        "\n",
        "    input:\n",
        "        x: training data -- array with shape (n_examples x n_features)\n",
        "        y: training data class label -- flat array with length (n_examples)\n",
        "        alpha: regularization strength -- float\n",
        "\n",
        "    output:\n",
        "        m0: mean of the training examples of class 0 -- flat array with length (n_features)\n",
        "        m1: mean of the training examples of class 1 -- flat array with length (n_features)\n",
        "        mdiff: difference of the two class means -- flat array with length (n_features)\n",
        "        w: regularized LDA weight vector -- flat array with length (n_features)\n",
        "        '''\n",
        "\n",
        "    # ---------------- INSERT CODE ----------------------\n",
        "\n",
        "\n",
        "\n",
        "    # ---------------- END CODE -------------------------\n",
        "\n",
        "    return m0, m1, mdiff, w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# split dataset and apply LDA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# visualize LDA weights using stemplot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2.2\n",
        "\n",
        "Project the training data and the test data on $\\vec{w}$. Visualize the class separation using a two-color histogram. \n",
        "- Is the class separation good?\n",
        "- Is there a big difference between training and test data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# example usage of histogram\n",
        "a, b = np.random.normal(2, 1, 20), np.random.normal(0, 1, 20)  # example data\n",
        "\n",
        "plt.title('Training data projection')\n",
        "_ = plt.hist(a,label='class 0',alpha=0.5)\n",
        "_ = plt.hist(b,label='class 1',alpha=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compute projections for training and test data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# visualize projections using histogram\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2.3\n",
        "\n",
        "Now we make class predictions based on the projections. Read https://en.wikipedia.org/wiki/Linear_discriminant_analysis#Fisher's_linear_discriminant and compute threshold $c$ for the projected values $p$ based on the training data. Print the value of $c$ and plot $c$ into the histograms of projected values you made before!\n",
        "\n",
        "Use $c$ to assign class labels for training and test set. Determine the classification errors (in terms of accuracy) on both datasets and print them. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_threshold_LDA(m0, m1, w):\n",
        "    '''Compute the optimal threshold for LDA .\n",
        "\n",
        "    input:\n",
        "        m0, m1: mean vectors of the two classes -- flat array with length (n_features)\n",
        "        w: weight vector of LDA -- flat array with length (n_features)\n",
        "\n",
        "    output:\n",
        "        c: the optimal threshold for LDA \n",
        "    '''\n",
        "\n",
        "    # ---------------- INSERT CODE ----------------------\n",
        "\n",
        "\n",
        "\n",
        "    # ---------------- END CODE -------------------------\n",
        "  \n",
        "    return c\n",
        "\n",
        "\n",
        "def score_LDA(x_proj, y, c):\n",
        "    '''Compute the accuracy given a threshold.\n",
        "\n",
        "    input:\n",
        "        x_proj: projected data -- flat array with length (n_examples)    \n",
        "        y: class labels -- flat array with length (n_examples)\n",
        "        c: optimal threshold for LDA - scalar\n",
        "\n",
        "    output:\n",
        "        y_hat: class label predicitons of LDA model -- flat array with length (n_examples)\n",
        "        acc: classification accuracy\n",
        "    '''\n",
        "    # ---------------- INSERT CODE ----------------------\n",
        "\n",
        "\n",
        "\n",
        "    # ---------------- END CODE -------------------------\n",
        "    \n",
        "    return y_hat, acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compute optimal threshold\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compute classification accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# visualize treshold in histogram\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 3: Logistic Regression\n",
        "\n",
        "Next, we will consider classification using Logistic Regression. \n",
        "\n",
        "For this task, we will use a different dataset:  \n",
        "It consists of activations from a convolutional neural network (ResNet18) for images of cats and dogs.\n",
        "The dataset contains 2,000 samples (i.e. CNN activations) and 256 features (i.e. the CNN activations have 256 dimensions). A target value of 0 indicates a cat, 1 a dog.\n",
        "\n",
        "Below, you find all imports that are necessary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_all, t_all = pickle.load(open('data/cnn_features.pickle', 'rb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 3.0: Normalize and split the data\n",
        "\n",
        "Make sure the data has has zero mean and variance 1 per feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# normalize data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 3.1: Iterative Reweighted Least Squares**\n",
        "\n",
        "1. Implement the IRLS algorithm and output at each iteration the current training accuracy. Remember the weight are updated according to:\n",
        "$$ w' = w - ( \\Phi^T R \\Phi ) ^ {-1} \\Phi^T (y - t)$$\n",
        "where $y$ is the prediction, $t$ the ground truth target, $R$ the weighting matrix and $\\Phi$ the design matrix.  \n",
        "\n",
        "    Hints:  \n",
        "    (a) There is a bias term in logistic regression  \n",
        "    (b) Use a small value for weight init to avoid numerical problems.\n",
        "\n",
        "\n",
        "2. Apply the IRLS algorithm on data and compute the test accuracy.\n",
        "3. Compare the results of your implementation to the sklearn implementation of `LogisticRegression(penalty='none')`. Do you get the same result?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IRLS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 3.2: Logistic Regression with Regularization\n",
        "\n",
        "1. Set sklearn's penalty parameter to `l1` and `l2`. Use the range `np.linspace(0.02, 1, 25)` for the parameter `C`, which controls the strength of regularization. Where is the regularization strongest, for small or big `C`?   \n",
        "Hint: For `l1` regularization you can use the `saga` solver.\n",
        "2. Plot strength of regularization over accuracy. Does regularization improve the scores?\n",
        "3. Visualize the coefficients (or just a subset of all coefficient for a better overview) of the regularized settings and the unregularized setting. What do you observe?\n",
        "3. Compare the coefficients to the LDA weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
    },
    "kernelspec": {
      "display_name": "Python 3.9.10 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "nteract": {
      "version": "0.28.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
