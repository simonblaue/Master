{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Practical 4\n",
    "\n",
    "Names: {YOUR NAMES}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cvxopt\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0: The Data\n",
    "\n",
    "We will work with the data from Practical 3. Load the data and split it into a training and test set. You can re-use the data splitting function from Practical 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data in training and test set\n",
    "def split_data(X, y, frac=0.3, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    # ---------------- INSERT CODE ----------------------\n",
    "    idx = X.shape[0]\n",
    "\n",
    "    n_trainset = int(idx*(1-frac))  # size of the training set\n",
    "    n_testset = idx-n_trainset  # size of the test set\n",
    "\n",
    "    idx_shuffled = np.random.permutation(idx)\n",
    "\n",
    "    test_idx = idx_shuffled[:n_testset]\n",
    "    train_idx = idx_shuffled[n_testset:n_testset+n_trainset]\n",
    "\n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    print('Test set shapes (X and y)', X_test.shape, y_test.shape)\n",
    "\n",
    "    X_train = X[train_idx]\n",
    "    y_train = y[train_idx]\n",
    "    print('Training set shapes (X and y):', X_train.shape, y_train.shape)\n",
    "    \n",
    "\n",
    "    # ---------------- END CODE -------------------------\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "X_2d, t_2d = np.load('nonlin_2d_data.npy')[:,:2], np.load('nonlin_2d_data.npy')[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set shapes (X and y) (75, 2) (75,)\n",
      "Training set shapes (X and y): (175, 2) (175,)\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "X_train, X_test, t_train, t_test = split_data(X_2d, t_2d, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Support Vector Machines\n",
    "\n",
    "First, you will implement a training algorithm for the Support Vector Machine (SVM). For solving the quadratic program, we provide a simple interface to the cvxopt library below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SVMs, each data sample $x_n$ has a corresponding lagrange multiplier $\\alpha_n$ which indicates if $x_n$ is a support vector. In the latter case $\\alpha_n > 0$ holds. \n",
    "The goal of learning the SVM is to figure out which samples are support vectors by learning $\\mathbf{\\alpha}$. The dual SVM optimizes the following quadratic program.\n",
    "\n",
    "$$ \\min \\frac{1}{2} \\sum_{n=1}^N \\sum_{m=1}^N \\alpha_n \\alpha_m t_n t_m k(\\mathbf{x}_n, \\mathbf{x}_m) - \\sum_{n=1}^N \\alpha_n$$\n",
    "subject to \n",
    "$$ 0 \\leq \\alpha_n \\leq C $$\n",
    "$$ \\sum_{n=1}^N \\alpha_n t_n = 0 $$ \n",
    "\n",
    "The quadratic program solver expects the following form:\n",
    "$$ \\min \\frac{1}{2} \\alpha^T P \\alpha + \\mathbf q^T \\mathbf \\alpha $$\n",
    "subject to \n",
    "\n",
    "$$A \\alpha = b$$\n",
    "$$G \\alpha \\leq h $$\n",
    "\n",
    "Here, $A$ and $G$ are matrices with one row per individual constraint. Similarly, $b$ (the intercept or bias) and $h$ are vectors with one element per individual constraint.\n",
    "\n",
    "Having trained the SVM, a prediction for an input $\\mathbf{x}$ is made by:\n",
    "\n",
    "$$ y = sign([\\sum_n^{N} \\alpha_n t_n k(\\mathbf{x}, \\mathbf{x}_n)] + b)  $$\n",
    "\n",
    "\n",
    "### Task 1.1\n",
    " \n",
    "Use the code provided below as a basis to express the constrained optimization problem in terms of $P, q, A, b, G$ and $h$ and implement a function `fit_svm` which passes these variables to the provided QP solver. Fit a SVM on the training data and extract its parameters using a linear kernel (dot product).\n",
    "\n",
    "**Hints:**\n",
    "  - The box constraint $0 \\leq \\alpha_n \\leq C$ defines two constraints of the form $G \\alpha_n \\leq h$ for each $\\alpha_n$.\n",
    "  - The inequality $x \\geq 0$ is equivalent to $-x \\leq 0$.\n",
    "  - The SVM is described in chapter 12 of Elements of Statistical Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel(a, b):\n",
    "    # ---------------- INSERT CODE ----------------------\n",
    "    return a @ b\n",
    "    # ---------------- END CODE -------------------------\n",
    "    \n",
    "def fit_svm(X, t, kernel, C=1.0):\n",
    "    '''Fit SVM using data (X,t), specified kernel and parameter C.\n",
    "    Inputs\n",
    "        X:  predictors\n",
    "        t:  targets\n",
    "        C:  constant\n",
    "    '''\n",
    "\n",
    "    t = np.array([-1. if l == 0 else 1. for l in t])\n",
    "    # ---------------- INSERT CODE ----------------------\n",
    "    N = len(t)\n",
    "    P = np.zeros((len(X),len(X)))\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            P[i][j]=t[i]*t[j]*kernel(X[i],X[j])\n",
    "    q = -1*np.ones(N)\n",
    "    A = t.reshape(1,N)\n",
    "    print(np.shape(A))\n",
    "    b = np.zeros(1).reshape(1,1)\n",
    "    G = np.zeros(2*N*N)\n",
    "    G = G.reshape(2*N,N)\n",
    "    for i in range(N):\n",
    "         G[i,i]=1\n",
    "    h = np.array([C for i in range(N)])\n",
    "    h = np.append(h,np.zeros(N))     \n",
    "    # ---------------- END CODE -------------------------\n",
    "    \n",
    "    assert P.shape == (len(X), len(X))\n",
    "    assert len(q) == len(X)\n",
    "    assert A.shape == (1, len(t)) and A.dtype == 'float'\n",
    "    assert b.shape == (1, 1)\n",
    "    assert len(G) == 2 * len(X)\n",
    "    assert len(h) == 2 * len(X)\n",
    "\n",
    "    return solve_quadratic_program(P, q, A, b, G, h)\n",
    "\n",
    "def solve_quadratic_program(P, q, A, b, G, h):\n",
    "    '''Uses cvxopt to solve the quadratic program.'''\n",
    "    P, q, A, b, G, h = [cvxopt.matrix(var) for var in [P, q, A, b, G, h]]\n",
    "    minimization = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "    lagr_mult = np.ravel(minimization['x'])\n",
    "    return lagr_mult\n",
    "\n",
    "\n",
    "def extract_parameters(X, t, kernel, lagr_mult, threshold=1e-7):\n",
    "    '''Computes the intercept from the support vector constraints.\n",
    "    \n",
    "    Inputs\n",
    "        X:         predictors\n",
    "        t:         targets\n",
    "        kernel:    a kernel to be used\n",
    "        lagr_mult: the Lagrange multipliers obtained by solving the dual QP\n",
    "        threshold: threshold for choosing support vectors\n",
    "    \n",
    "    Returns\n",
    "        lagr_mult: lagrange multipliers for the support vectors\n",
    "        svs:       set of support vectors\n",
    "        sv_labels: targets t_n for the support vectors\n",
    "        intercept: computed intercept (also called bias)\n",
    "    '''\n",
    "    t = np.array([-1. if l == 0 else 1. for l in t])\n",
    "    \n",
    "    # ---------------- INSERT CODE ----------------------\n",
    "    A = t.reshape((1,len(t)))\n",
    "    intercept = A @ lagr_mult\n",
    "    svs_indices = np.where(lagr_mult < threshold)\n",
    "    svs = X[svs_indices]\n",
    "    sv_labels = np.sign(svs)\n",
    "    lagr_mult = lagr_mult[svs_indices]\n",
    "    # ---------------- END CODE -------------------------\n",
    "    \n",
    "\n",
    "    return lagr_mult, svs, sv_labels, intercept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 124)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -2.7068e+01 -2.4223e+02  2e+03  3e+00  4e+00\n",
      " 1: -1.7354e+00 -1.4036e+02  1e+02  3e-02  4e-02\n",
      " 2: -1.6271e+01 -5.6256e+01  4e+01  7e-03  9e-03\n",
      " 3: -3.2577e+01 -4.9012e+01  2e+01  2e-03  3e-03\n",
      " 4: -3.8620e+01 -4.6233e+01  9e+00  8e-04  1e-03\n",
      " 5: -4.2191e+01 -4.4319e+01  3e+00  2e-04  3e-04\n",
      " 6: -4.3305e+01 -4.3923e+01  7e-01  4e-05  5e-05\n",
      " 7: -4.3656e+01 -4.3816e+01  2e-01  9e-06  1e-05\n",
      " 8: -4.3770e+01 -4.3778e+01  9e-03  4e-07  5e-07\n",
      " 9: -4.3776e+01 -4.3776e+01  1e-04  4e-09  7e-09\n",
      "10: -4.3776e+01 -4.3776e+01  1e-06  4e-11  4e-09\n",
      "Optimal solution found.\n",
      "(12, 13) (12, 13) [-3.55271368e-15]\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# Fit SVM on training data\n",
    "lagrange_multiplier = fit_svm(X_train,t_train, linear_kernel)\n",
    "\n",
    "# Extract parameters\n",
    "lagr_mult , svs,sv_labels, intercept = extract_parameters(X_train, t_train, linear_kernel, lagrange_multiplier)\n",
    "\n",
    "print(svs.shape,sv_labels.shape, intercept)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2\n",
    "\n",
    "Having learnt an SVM, we can use the calculated parameters to make predictions on novel samples.\n",
    "- Implement a function `svm_predict(X, kernel, lagr_mult, svs, sv_labels, intercept)`.\n",
    "- Use this function with the linear kernel and compute the test accuracy on the 2d dataset.\n",
    "- Visualize the samples form the test set in a scatter plot colored by your predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def svm_predict(X, kernel, lagr_mult, svs, sv_labels, intercept):\n",
    "    ''' Given the learned parameters of the SVM, make a prediction on the test set.\n",
    "    Inputs\n",
    "        X:         predictors\n",
    "        kernel:    a kernel to be used\n",
    "        lagr_mult: the Lagrange multipliers obtained by solving the dual QP\n",
    "        svs:       set of support vectors\n",
    "        sv_labels: targets t_n for the support vectors\n",
    "        intercept: computed intercept (also called bias)\n",
    "    \n",
    "    Returns\n",
    "        prediction: predictions on novel samples\n",
    "    '''  \n",
    "    \n",
    "    # ---------------- INSERT CODE ----------------------\n",
    "    K = np.array([kernel(sv , xv) for sv in svs for xv in X ]).reshape(svs.shape[0],X.shape[0])\n",
    "    print(K)\n",
    "    print(svs.shape)\n",
    "    prediction = [(alpha_n * t_n) * K for alpha_n,t_n in zip(lagr_mult,sv_labels)] \n",
    "    # ---------------- END CODE -------------------------\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2129072.2259     1069106.6614      819162.2308      840816.6742\n",
      "   706433.5582     1766604.7593     1666268.35       2548733.9991\n",
      "  1749713.0567     1254090.6177     1430563.4732     2189244.0056\n",
      "  2213479.6425      476870.0722     1942433.1073     1592209.1598\n",
      "  1324567.0563     2448653.1126     1059921.2345     2557259.5068\n",
      "  1153114.2592     1138718.0134     1002759.0307     1826992.2539\n",
      "  1911447.7231     1894756.9573     1086920.9107      707028.2868\n",
      "   747009.7622     2171040.5569      692122.1797     1248160.0588\n",
      "  1228692.8257     1674984.0523     2178101.101       815676.3301\n",
      "  1071205.6991     1424784.097      1499972.2956      884720.4752\n",
      "  1209371.8206     1203800.882       852615.2455     1219792.1891\n",
      "  1150176.8104     1852061.2824      728721.6421      952035.3447\n",
      "   829523.4567      634103.9627     1408465.4175     1153801.0749\n",
      "   865798.3032     1102581.1385    ]\n",
      " [ 720246.8298      364666.6691      282382.6671      287338.3604\n",
      "   242386.8793      598169.6837      564912.4812      860432.8904\n",
      "   592471.0617      426665.9918      485760.801       741298.9118\n",
      "   748759.3756      165988.8708      656602.9779      544492.5211\n",
      "   452378.2952      827408.8234      361078.5864      863363.8595\n",
      "   392793.8098      387348.8509      343050.8966      619433.5998\n",
      "   647975.7478      642470.8735      371281.9859      242988.4506\n",
      "   257243.1304      734274.861       238138.3537      426316.1351\n",
      "   418044.934       568053.2246      735760.724       278966.3561\n",
      "   366750.8327      485519.2558      509334.8788      303276.4261\n",
      "   411034.0704      409926.1645      293513.4192      414759.049\n",
      "   392068.6581      627737.5325      250130.7175      325864.162\n",
      "   283871.1258      218126.6012      480421.0661      393454.6288\n",
      "   295537.679       375774.4103    ]\n",
      " [ 550259.0782      279239.7855      216865.7374      220130.1344\n",
      "   185900.8068      457111.917       431846.574       656968.8278\n",
      "   452755.9866      326477.0137      371490.9276      566489.4174\n",
      "   572033.1284      127802.5547      501529.6146      417234.8865\n",
      "   346532.4352      631926.7934      276388.1135      659219.2489\n",
      "   300651.2797      296370.3788      262891.471       473525.7654\n",
      "   495328.411       491149.5405      284422.134       186441.2288\n",
      "   197487.1272      560937.3866      182774.7143      326561.6803\n",
      "   319880.4794      434283.4549      561880.8332      213764.0476\n",
      "   281117.6794      371688.1099      389517.776       232539.9727\n",
      "   314423.3464      313746.1593      225322.4931      317307.9156\n",
      "   300165.4644      479833.0786      191863.3149      249758.3067\n",
      "   217553.8067      167415.1535      367862.851       301266.4482\n",
      "   226344.1539      287676.0023    ]\n",
      " [ 548746.1743      279339.004       217818.5226      220390.9011\n",
      "   186403.344       455992.0127      431003.0972      654644.5932\n",
      "   451662.8196      326303.6008      371000.9265      565107.6155\n",
      "   570438.9449      128808.335       499988.6094      417710.1908\n",
      "   346777.4038      629892.8943      276396.7087      656912.6875\n",
      "   300613.6589      296191.4342      263304.7417      472612.4827\n",
      "   494356.0959      490229.5207      284693.8453      187040.0034\n",
      "   198262.0256      559336.3606      183425.7803      326832.637\n",
      "   319728.5351      433485.6928      560011.7944      214064.8273\n",
      "   281642.4779      371689.5636      389007.0324      233081.2315\n",
      "   314125.1292      313653.3225      226147.6655      317070.5555\n",
      "   300199.5616      478852.5179      192373.877       250206.8997\n",
      "   217929.246       168001.3559      367998.0326      301326.9155\n",
      "   226514.9079      287727.1341    ]\n",
      " [ 665372.6944      336509.4339      260222.403       265120.9241\n",
      "   223545.2087      552514.3368      521706.0138      795105.9839\n",
      "   547257.921       393898.104       448534.5259      684698.9456\n",
      "   691705.3486      152757.8379      606628.4132      502137.2369\n",
      "   417305.9188      764464.8893      333312.1936      797815.8598\n",
      "   362525.284       357597.1382      316464.5832      572059.164\n",
      "   598429.2832      593323.99        342554.6233      224005.2232\n",
      "   237073.1966      678339.2648      219486.3624      393297.745\n",
      "   385947.6747      524578.3873      679836.3979      257348.9427\n",
      "   338293.5567      448076.7279      470289.4584      279706.5651\n",
      "   379496.9028      378342.1553      270468.0433      382930.4895\n",
      "   361814.484       579746.6327      230610.9418      300599.0849\n",
      "   261877.1635      201065.3777      443307.1511      363056.72\n",
      "   272727.2029      346845.6423    ]\n",
      " [ 597819.5089      302938.7179      234819.3836      238803.6118\n",
      "   201523.3654      496517.2653      468976.7457      714054.4678\n",
      "   491811.3857      354379.8148      403377.5626      615319.6563\n",
      "   621471.7603      138179.8588      544928.741       452275.5855\n",
      "   375747.9634      686661.6234      299978.0764      716502.1363\n",
      "   326279.0266      321743.249       285109.8321      514252.6439\n",
      "   537948.009       533390.6999      308477.2715      202030.7556\n",
      "   213910.1511      609441.1342      197995.9523      354170.6319\n",
      "   347241.871       471604.5528      610608.6554      231835.7631\n",
      "   304811.5475      403257.7291      422949.424       252112.1285\n",
      "   341365.8461      340464.9094      243992.5205      344489.1262\n",
      "   325670.9187      521136.205       207908.3004      270850.1893\n",
      "   235963.9721      181355.4025      399102.5786      326805.0445\n",
      "   245582.8509      312185.331     ]\n",
      " [ 576899.7681      292178.6968      226328.8126      230269.6631\n",
      "   194274.4949      479124.2517      452506.5329      689146.8575\n",
      "   474571.7629      341838.1698      389150.7971      593759.3627\n",
      "   599728.7829      133091.7611      525896.6164      436186.7756\n",
      "   362407.8704      662690.7017      289324.353       691502.4173\n",
      "   314702.9062      310347.8228      274908.9091      496182.5597\n",
      "   519047.9592      514641.7045      297493.4844      194747.0304\n",
      "   206176.2645      588124.207       190851.5607      341564.7289\n",
      "   334942.6253      455028.0481      589298.2431      223549.2554\n",
      "   293899.074       388971.9766      408029.9524      243058.7066\n",
      "   329303.2437      328408.6303      235202.974       332298.4445\n",
      "   314116.9637      502832.3567      200445.2458      261148.351\n",
      "   227506.115       174819.2978      384914.5705      315215.9669\n",
      "   236828.6888      301096.6855    ]\n",
      " [ 602115.7188911   304763.7145962   235949.6260892   240142.58879695\n",
      "   202567.5488951   500026.8055948   472208.1966939   719341.69049425\n",
      "   495265.3276953   356646.9214923   406026.42929648  619652.11299495\n",
      "   625936.1549927   138622.6965975   548913.2885928   454897.9642974\n",
      "   378006.48979435  691699.29359325  301828.81729235  721801.7963931\n",
      "   328270.76879735  323758.80769706  286688.01539508  517775.28729476\n",
      "   541643.6354934   537033.22339375  310298.1181924   203015.89339794\n",
      "   214905.8675972   613828.4719932   198944.2514977   356244.06109568\n",
      "   349447.8636921   474813.44139616  615099.07879475  233123.0387976\n",
      "   306499.8760915   405788.6562907   425720.43149679  253436.62009565\n",
      "   343564.3715961   342589.8046967   245180.95649735  346680.54139615\n",
      "   327663.8349954   524715.9333958   208986.7963971   272332.4147943\n",
      "   237235.7708967   182248.9403966   401478.16859642  328803.05929673\n",
      "   247012.72969532  314104.91479158]\n",
      " [ 659953.6195      334137.245       258745.3979      263320.9146\n",
      "   222142.7555      548078.4479      517606.9638      788419.808\n",
      "   542870.665       390988.718       445110.0898      679215.8084\n",
      "   686070.9443      152088.0634      601622.4096      498780.4748\n",
      "   414432.3192      758125.7876      330908.2207      791113.6124\n",
      "   359913.1647      354952.5155      314364.7862      567576.0499\n",
      "   593726.5918      588681.3952      340204.2455      222653.4935\n",
      "   235705.4859      672797.1227      218193.0119      390602.0231\n",
      "   383100.8169      520485.1604      674169.569       255628.688\n",
      "   336079.6039      444860.579       466704.3193      277923.5208\n",
      "   376644.4873      375596.3525      268895.649       380081.1827\n",
      "   359235.2273      575180.5478      229175.3234      298622.7086\n",
      "   260148.4763      199870.143       440195.2712      360487.9438\n",
      "   270831.0022      344361.3932    ]\n",
      " [ 665098.4476      336254.4373      259881.8363      264836.0192\n",
      "   223246.5824      552296.9563      521461.9444      794837.4644\n",
      "   547019.8825      393578.6544      448254.5111      684438.7194\n",
      "   691441.116       152500.1916      606422.3006      501874.8604\n",
      "   417063.7655      764226.5419      332985.9071      797531.4174\n",
      "   362252.882       357315.8517      316127.4285      571776.1883\n",
      "   598135.4634      593025.2705      342257.9306      223741.1166\n",
      "   236784.2629      678084.6324      219232.5928      392999.5161\n",
      "   385611.5014      524329.7442      679610.9802      257093.2078\n",
      "   337920.7539      447766.7003      470004.335       279375.924\n",
      "   379230.4719      378094.6656      270214.4495      382628.1148\n",
      "   361555.1807      579475.2062      230384.8774      300259.9008\n",
      "   261568.9102      200829.1142      442966.3992      362830.0142\n",
      "   272450.9537      346519.8013    ]\n",
      " [1960759.9281      984670.5369      754542.4351      774409.1609\n",
      "   650667.363      1626973.3649     1534582.8661     2347191.3346\n",
      "  1611409.4394     1155004.3371     1317512.7747     2016202.0084\n",
      "  2038495.5421      439295.3386     1788872.165      1466533.4934\n",
      "  1219997.1947     2255059.2136      976178.8546     2355038.7848\n",
      "  1062021.8346     1048740.5717      923573.1955     1682599.6738\n",
      "  1760373.9311     1745006.4209     1001087.9496      651231.0905\n",
      "   688074.0901     1999408.668       637518.0986     1149602.035\n",
      "  1131606.8216     1542612.7739     2005888.1657      751268.5053\n",
      "   986625.8071     1312247.594      1381434.0154      814871.4729\n",
      "  1113808.9801     1108710.2772      785362.9754     1123403.3981\n",
      "  1059332.1602     1705679.9835      671216.3636      876856.1592\n",
      "   764014.6029      584075.9696     1297219.2158     1062681.9527\n",
      "   797410.1218     1015476.9007    ]\n",
      " [ 634039.0514      320917.6199      248384.5761      252880.0596\n",
      "   213282.3805      526546.037       497244.7829      757528.23\n",
      "   521540.96        375528.4167      427574.4743      652529.5665\n",
      "   659135.5456      145960.7196      578019.5925      479014.7617\n",
      "   398021.414       728396.8692      317804.1826      760112.219\n",
      "   345694.5461      340943.1423      301878.8609      545239.0058\n",
      "   570370.086       565518.0962      326714.8216      213781.8507\n",
      "   226294.3568      646390.5208      209488.6778      375126.724\n",
      "   367950.8023      500006.5443      647740.7691      245487.6099\n",
      "   322710.0064      427259.6277      448319.8194      266862.7977\n",
      "   361790.5702      360759.5648      258164.6024      365069.2534\n",
      "   345031.2016      552560.7261      220057.6735      286755.4491\n",
      "   249818.6641      191890.7251      422779.3405      346231.6204\n",
      "   260103.3128      330724.7704    ]]\n",
      "(12, 13)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (13,) (12,54) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/simonblaue/ownCloud/Master/Machine Learning/Practical 4/4_[NAME1]_[NAME2]_[NAME3].ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/simonblaue/ownCloud/Master/Machine%20Learning/Practical%204/4_%5BNAME1%5D_%5BNAME2%5D_%5BNAME3%5D.ipynb#ch0000012?line=0'>1</a>\u001b[0m \u001b[39m# Testing\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/simonblaue/ownCloud/Master/Machine%20Learning/Practical%204/4_%5BNAME1%5D_%5BNAME2%5D_%5BNAME3%5D.ipynb#ch0000012?line=1'>2</a>\u001b[0m \u001b[39m# make predictions for test set\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/simonblaue/ownCloud/Master/Machine%20Learning/Practical%204/4_%5BNAME1%5D_%5BNAME2%5D_%5BNAME3%5D.ipynb#ch0000012?line=2'>3</a>\u001b[0m test_predictions \u001b[39m=\u001b[39m svm_predict(X_test,linear_kernel,lagr_mult, svs, sv_labels, intercept)\n",
      "\u001b[1;32m/Users/simonblaue/ownCloud/Master/Machine Learning/Practical 4/4_[NAME1]_[NAME2]_[NAME3].ipynb Cell 12'\u001b[0m in \u001b[0;36msvm_predict\u001b[0;34m(X, kernel, lagr_mult, svs, sv_labels, intercept)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/simonblaue/ownCloud/Master/Machine%20Learning/Practical%204/4_%5BNAME1%5D_%5BNAME2%5D_%5BNAME3%5D.ipynb#ch0000011?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(K)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/simonblaue/ownCloud/Master/Machine%20Learning/Practical%204/4_%5BNAME1%5D_%5BNAME2%5D_%5BNAME3%5D.ipynb#ch0000011?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(svs\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/simonblaue/ownCloud/Master/Machine%20Learning/Practical%204/4_%5BNAME1%5D_%5BNAME2%5D_%5BNAME3%5D.ipynb#ch0000011?line=18'>19</a>\u001b[0m prediction \u001b[39m=\u001b[39m [(alpha_n \u001b[39m*\u001b[39m t_n) \u001b[39m*\u001b[39m K \u001b[39mfor\u001b[39;00m alpha_n,t_n \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(lagr_mult,sv_labels)] \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/simonblaue/ownCloud/Master/Machine%20Learning/Practical%204/4_%5BNAME1%5D_%5BNAME2%5D_%5BNAME3%5D.ipynb#ch0000011?line=19'>20</a>\u001b[0m \u001b[39m# ---------------- END CODE -------------------------\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/simonblaue/ownCloud/Master/Machine%20Learning/Practical%204/4_%5BNAME1%5D_%5BNAME2%5D_%5BNAME3%5D.ipynb#ch0000011?line=21'>22</a>\u001b[0m \u001b[39mreturn\u001b[39;00m prediction\n",
      "\u001b[1;32m/Users/simonblaue/ownCloud/Master/Machine Learning/Practical 4/4_[NAME1]_[NAME2]_[NAME3].ipynb Cell 12'\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/simonblaue/ownCloud/Master/Machine%20Learning/Practical%204/4_%5BNAME1%5D_%5BNAME2%5D_%5BNAME3%5D.ipynb#ch0000011?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(K)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/simonblaue/ownCloud/Master/Machine%20Learning/Practical%204/4_%5BNAME1%5D_%5BNAME2%5D_%5BNAME3%5D.ipynb#ch0000011?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(svs\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/simonblaue/ownCloud/Master/Machine%20Learning/Practical%204/4_%5BNAME1%5D_%5BNAME2%5D_%5BNAME3%5D.ipynb#ch0000011?line=18'>19</a>\u001b[0m prediction \u001b[39m=\u001b[39m [(alpha_n \u001b[39m*\u001b[39;49m t_n) \u001b[39m*\u001b[39;49m K \u001b[39mfor\u001b[39;00m alpha_n,t_n \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(lagr_mult,sv_labels)] \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/simonblaue/ownCloud/Master/Machine%20Learning/Practical%204/4_%5BNAME1%5D_%5BNAME2%5D_%5BNAME3%5D.ipynb#ch0000011?line=19'>20</a>\u001b[0m \u001b[39m# ---------------- END CODE -------------------------\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/simonblaue/ownCloud/Master/Machine%20Learning/Practical%204/4_%5BNAME1%5D_%5BNAME2%5D_%5BNAME3%5D.ipynb#ch0000011?line=21'>22</a>\u001b[0m \u001b[39mreturn\u001b[39;00m prediction\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (13,) (12,54) "
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "# make predictions for test set\n",
    "test_predictions = svm_predict(X_test,linear_kernel,lagr_mult, svs, sv_labels, intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3\n",
    "\n",
    "- Instead of using the linear kernel, use the Gaussian RBF kernel defined in Practical 3.\n",
    "- Compare results on with both kernels with sklearn implementation (SVC)\n",
    "- Visualize the predictions on the test set, the learned support vectors and the decision boundary for both kernels (Hint: Adapt the decision boundary plot from Practical 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(a, b):\n",
    "    # ---------------- INSERT CODE ----------------------\n",
    "    return np.exp(np.sqrt((a-b)**2))\n",
    "\n",
    "\n",
    "    # ---------------- END CODE -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit SVM with rbf kernel and calculate the test accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit SVM using sklearn and calculate the test accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Decision Trees\n",
    "\n",
    "Next, we will implement a simple decision tree classifier using the Wine dataset, one of the standard sklearn datasets. \n",
    "\n",
    "We will use the Gini impurity as a criterion for splitting. It is defined for a set of labels as\n",
    "$$ G = \\sum_{i=0}^C p(i) * (1- p(i)) $$\n",
    "\n",
    "Given labels $l$ and split $l_a$ and $l_b$, the weighted removed impurity can be computed by $G(l) - \\frac{|l_a|}{|l|}G(l_a) - \\frac{|l_b|}{|l|}G(l_b)$.\n",
    "\n",
    "Here is a simple explanation of the Gini impurity that you may find useful: https://victorzhou.com/blog/gini-impurity/\n",
    "\n",
    "\n",
    "### Task 2.1\n",
    "\n",
    "1. Plot the distribution of the first feature of for each class of the wine dataset.\n",
    "2. Implement a function `gini_impurity(t)` that computes the Gini impurity for an array of labels `t`.\n",
    "3. Calculate the removed Gini impurity for a split after 50 samples, i.e. between `t[:50]` and `t[50:]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set shapes (X and y) (54, 13) (54,)\n",
      "Training set shapes (X and y): (124, 13) (124,)\n"
     ]
    }
   ],
   "source": [
    "# Load Wine dataset and split into train+test set\n",
    "\n",
    "X, t = load_wine(return_X_y=True)\n",
    "X_train, X_test, t_train, t_test = split_data(X, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Gini impurity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2\n",
    "For each of the first 12 features, compute the remove Gini impurity for every possible split. Visualize the removed Gini impurity per feature across all splits. Which is the optimal split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3\n",
    "\n",
    "1. Implement a function `build_tree(X, t, depth)` which recursively builds a tree. Use the classes `Node` and `Leaf` as a data structure to build your tree.\n",
    "2. Implement a function `predict_tree(tree, x)` which makes a prediction for sample `x`. Obtain scores for the `wine` dataset and compare to `sklearn.tree.DecisionTree`.\n",
    "3. Switch back to the synthetic 2d dataset from the beginning (kernel methods). Compute scores and visualize the decisions in a 2d grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, left, right, n_feat, threshold):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.n_feat = n_feat\n",
    "        self.threshold = threshold\n",
    "\n",
    "\n",
    "class Leaf:\n",
    "    def __init__(self, label):\n",
    "        self.label = label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (3238061611.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [19]\u001b[0;36m\u001b[0m\n\u001b[0;31m    def predict_tree(node, x):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# Implement recursive tree function\n",
    "\n",
    "def build_tree(X, t, depth, max_depth=3, n_labels=2):\n",
    "    \n",
    "    # ---------------- INSERT CODE ----------------------\n",
    "\n",
    "\n",
    "\n",
    "    # ---------------- END CODE -------------------------\n",
    "\n",
    "    \n",
    "def predict_tree(node, x):\n",
    "    \n",
    "    # ---------------- INSERT CODE ----------------------\n",
    "\n",
    "\n",
    "\n",
    "    # ---------------- END CODE -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build tree\n",
    "\n",
    "tree = build_tree(X_train, t_train, 0, max_depth=3, n_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate training and test scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate test score using sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate test score for synthetic 2D dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "nteract": {
   "version": "0.28.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
