{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Practical 4\n",
    "\n",
    "Names: {YOUR NAMES}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cvxopt\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0: The Data\n",
    "\n",
    "We will work with the data from Practical 3. Load the data and split it into a training and test set. You can re-use the data splitting function from Practical 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, frac=0.3, seed=1):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    # ---------------- INSERT CODE ----------------------\n",
    "    idx = X.shape[0]\n",
    "\n",
    "    n_trainset = int(idx*(1-frac))  # size of the training set\n",
    "    n_testset = idx-n_trainset  # size of the test set\n",
    "\n",
    "    idx_shuffled = np.random.permutation(idx)\n",
    "\n",
    "    test_idx = idx_shuffled[:n_testset]\n",
    "    train_idx = idx_shuffled[n_testset:n_testset+n_trainset]\n",
    "\n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "\n",
    "    X_train = X[train_idx]\n",
    "    y_train = y[train_idx]\n",
    "    \n",
    "\n",
    "    # ---------------- END CODE -------------------------\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "X_2d, t_2d = np.load('data/nonlin_2d_data.npy')[:,:2], np.load('data/nonlin_2d_data.npy')[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "X_train, X_test, t_train, t_test = split_data(X_2d, t_2d, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Support Vector Machines\n",
    "\n",
    "First, you will implement a training algorithm for the Support Vector Machine (SVM). For solving the quadratic program, we provide a simple interface to the cvxopt library below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SVMs, each data sample $x_n$ has a corresponding lagrange multiplier $\\alpha_n$ which indicates if $x_n$ is a support vector. In the latter case $\\alpha_n > 0$ holds. \n",
    "The goal of learning the SVM is to figure out which samples are support vectors by learning $\\mathbf{\\alpha}$. The dual SVM optimizes the following quadratic program.\n",
    "\n",
    "$$ \\min \\frac{1}{2} \\sum_{n=1}^N \\sum_{m=1}^N \\alpha_n \\alpha_m t_n t_m k(\\mathbf{x}_n, \\mathbf{x}_m) - \\sum_{n=1}^N \\alpha_n$$\n",
    "subject to \n",
    "$$ 0 \\leq \\alpha_n \\leq C $$\n",
    "$$ \\sum_{n=1}^N \\alpha_n t_n = 0 $$ \n",
    "\n",
    "The quadratic program solver expects the following form:\n",
    "$$ \\min \\frac{1}{2} \\alpha^T P \\alpha + \\mathbf q^T \\mathbf \\alpha $$\n",
    "subject to \n",
    "\n",
    "$$A \\alpha = b$$\n",
    "$$G \\alpha \\leq h $$\n",
    "\n",
    "Here, $A$ and $G$ are matrices with one row per individual constraint. Similarly, $b$ (the intercept or bias) and $h$ are vectors with one element per individual constraint.\n",
    "\n",
    "Having trained the SVM, a prediction for an input $\\mathbf{x}$ is made by:\n",
    "\n",
    "$$ y = sign([\\sum_n^{N} \\alpha_n t_n k(\\mathbf{x}, \\mathbf{x}_n)] + b)  $$\n",
    "\n",
    "\n",
    "### Task 1.1\n",
    " \n",
    "Use the code provided below as a basis to express the constrained optimization problem in terms of $P, q, A, b, G$ and $h$ and implement a function `fit_svm` which passes these variables to the provided QP solver. Fit a SVM on the training data and extract its parameters using a linear kernel (dot product).\n",
    "\n",
    "**Hints:**\n",
    "  - The box constraint $0 \\leq \\alpha_n \\leq C$ defines two constraints of the form $G \\alpha_n \\leq h$ for each $\\alpha_n$.\n",
    "  - The inequality $x \\geq 0$ is equivalent to $-x \\leq 0$.\n",
    "  - The SVM is described in chapter 12 of Elements of Statistical Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel(a, b):\n",
    "    return a@b\n",
    "    # ---------------- INSERT CODE ----------------------\n",
    "\n",
    "\n",
    "\n",
    "    # ---------------- END CODE -------------------------\n",
    "\n",
    "\n",
    "def fit_svm(X, t, kernel, C=1.0):\n",
    "        #'''Fit SVM using data (X,t), specified kernel and parameter C.\n",
    "    #Inputs\n",
    "        #X:  predictors\n",
    "        #t:  targets\n",
    "        #C:  constant\n",
    "    #'''\n",
    "    t = np.array([-1. if l == 0 else 1. for l in t])\n",
    "    N=len(t)\n",
    "    # ---------------- INSERT CODE ----------------------\n",
    "    P =np.identity(N)\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            P[i][j]=t[i]*t[j]*kernel(X[i],X[j])\n",
    "    q = -1*np.ones(N)\n",
    "    A = t.reshape(1,N)\n",
    "    print(np.shape(A))\n",
    "    b = np.zeros(1).reshape(1,1)\n",
    "    G = np.zeros(2*N*N)\n",
    "    G=G.reshape(2*N,N)\n",
    "    for i in range(N):\n",
    "        G[i,i]=1\n",
    "        G[N+i,i]\n",
    "    h=np.array([C for i in range(N)])\n",
    "    h=np.append(h,np.zeros(N))     \n",
    "    # ---------------- END CODE -------------------------\n",
    "    n_samples=N\n",
    "    assert P.shape == (len(X), len(X))\n",
    "    assert len(q) == len(X)\n",
    "    assert A.shape == (1, n_samples) \n",
    "    assert A.dtype == 'float'\n",
    "    assert b.shape == (1, 1)\n",
    "    assert len(G) == 2 * len(X)\n",
    "    assert len(h) == 2 * len(X)\n",
    "\n",
    "    return solve_quadratic_program(P, q, A, b, G, h)\n",
    "\n",
    "def solve_quadratic_program(P, q, A, b, G, h):\n",
    "    '''Uses cvxopt to solve the quadratic program.'''\n",
    "    P, q, A, b, G, h = [cvxopt.matrix(var) for var in [P, q, A, b, G, h]]\n",
    "    minimization = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "    lagr_mult = np.ravel(minimization['x'])\n",
    "    return lagr_mult\n",
    "\n",
    "\n",
    "def extract_parameters(X, t, kernel, lagr_mult, threshold=1e-7):\n",
    "    '''Computes the intercept from the support vector constraints.\n",
    "    \n",
    "    Inputs\n",
    "        X:         predictors\n",
    "        t:         targets\n",
    "        kernel:    a kernel to be used\n",
    "        lagr_mult: the Lagrange multipliers obtained by solving the dual QP\n",
    "        threshold: threshold for choosing support vectors\n",
    "    \n",
    "    Returns\n",
    "        lagr_mult: lagrange multipliers for the support vectors\n",
    "        svs:       set of support vectors\n",
    "        sv_labels: targets t_n for the support vectors\n",
    "        intercept: computed intercept (also called bias)\n",
    "    '''\n",
    "    t = np.array([-1. if l == 0 else 1. for l in t])\n",
    "    \n",
    "    # ---------------- INSERT CODE ----------------------\n",
    "    N=len(t)\n",
    "    intercept=np.array(t)\n",
    "    \n",
    "    for i in range(N):\n",
    "        s=0\n",
    "        for j in range(N):\n",
    "            s+=t[j]*lagr_mult[j]*X[j]@X.T\n",
    "        intercept-=s\n",
    "    \n",
    "    intercept=np.mean(intercept)\n",
    "    svs=np.zeros(2)\n",
    "    sv_labels=np.zeros(N)\n",
    "     \n",
    "    for i in range(N):\n",
    "        svs+=lagr_mult[i]*t[i]*X[i]\n",
    "        sv_labels+=intercept+kernel(X[i],X.T)*lagr_mult*t\n",
    "    # ---------------- END CODE -------------------------\n",
    "\n",
    "    return lagr_mult, svs, sv_labels, intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 175)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.3599e+02 -2.7495e-05  9e+02  3e+00  1e+00\n",
      " 1: -1.0725e+02 -1.6784e+02  7e+01  3e-02  1e-02\n",
      " 2: -1.3984e+02 -1.4602e+02  7e+00  3e-03  9e-04\n",
      " 3: -1.3999e+02 -1.4006e+02  7e-02  3e-05  1e-05\n",
      " 4: -1.4000e+02 -1.4000e+02  7e-04  3e-07  1e-07\n",
      " 5: -1.4000e+02 -1.4000e+02  7e-06  3e-09  1e-09\n",
      "Optimal solution found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1.        , 1.        , 0.66750816, 0.66811915, 0.66722297,\n",
       "        1.        , 1.        , 0.6647988 , 1.        , 1.        ,\n",
       "        0.67068015, 1.        , 0.66118482, 1.        , 1.        ,\n",
       "        0.66495307, 0.66199543, 1.        , 0.66062643, 1.        ,\n",
       "        1.        , 1.        , 0.66233809, 0.6641014 , 1.        ,\n",
       "        0.66400874, 1.        , 0.99999999, 0.66094407, 0.66264546,\n",
       "        0.66116519, 1.        , 0.66383289, 1.        , 0.67725611,\n",
       "        0.66404356, 0.68073898, 0.65990356, 0.99999999, 1.        ,\n",
       "        0.6639249 , 0.66280827, 0.67420636, 1.        , 0.66445642,\n",
       "        1.        , 0.66335982, 0.66421049, 0.67569219, 0.6641338 ,\n",
       "        0.66005008, 0.66383152, 0.66461809, 1.        , 0.66165353,\n",
       "        0.68033624, 0.66413057, 1.        , 0.68101114, 0.67801923,\n",
       "        1.        , 0.66796331, 0.67939716, 0.99999999, 0.66414117,\n",
       "        0.6637172 , 0.66266996, 0.66356432, 1.        , 0.66077657,\n",
       "        0.66437101, 0.66064454, 1.        , 0.66225737, 1.        ,\n",
       "        0.66041328, 0.66052758, 1.        , 0.67014338, 1.        ,\n",
       "        0.66440999, 0.99999999, 1.        , 1.        , 0.6807501 ,\n",
       "        1.        , 0.67290143, 0.67865878, 1.        , 0.67672737,\n",
       "        1.        , 0.66407269, 0.65989776, 1.        , 0.67604158,\n",
       "        0.66462189, 0.65991395, 0.66462519, 0.67041127, 1.        ,\n",
       "        0.66390395, 0.99999999, 0.66828796, 0.66426667, 0.66992027,\n",
       "        0.67694863, 0.66933216, 0.66089724, 0.65974771, 1.        ,\n",
       "        1.        , 0.99999999, 0.68028819, 0.66136433, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.66028028, 1.        , 0.66789088, 0.66456026,\n",
       "        0.66478709, 0.67841375, 1.        , 0.65982912, 0.66071507,\n",
       "        0.66340801, 0.66451759, 0.66203838, 1.        , 0.6631129 ,\n",
       "        1.        , 0.66048933, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.67170938, 0.66434186,\n",
       "        0.99999999, 0.99999999, 0.66487881, 0.67886084, 1.        ,\n",
       "        0.99999999, 0.68146421, 0.66424871, 0.66679438, 0.66262308,\n",
       "        0.66224484, 0.66039447, 0.66056448, 1.        , 0.67758937,\n",
       "        1.        , 0.66038999, 1.        , 0.6769601 , 0.66063802,\n",
       "        0.66380258, 0.668334  , 0.99999999, 0.66109775, 0.67552629,\n",
       "        0.66387275, 1.        , 0.67848087, 0.66205464, 1.        ]),\n",
       " array([2.36218789e-09, 2.22818775e-09]),\n",
       " array([-298.96306204, -344.88383929,  -66.53457805,  -52.69720927,\n",
       "         -93.39217541, -286.19304815, -172.90159786,  465.78832053,\n",
       "        -189.79494112, -247.15730042,    5.52175486, -169.61668499,\n",
       "          20.03012018, -359.84093351, -187.92273899,  515.14399445,\n",
       "        -238.32771055,   13.35672615, -151.65029816, -236.50389558,\n",
       "        -275.38592068, -220.46355044,  263.76219026,  673.99012113,\n",
       "        -363.69508027,  612.68993822, -301.58644362,  206.20903094,\n",
       "         -19.46578019,  278.08649014, -258.83962619, -507.93662211,\n",
       "         399.39780096, -513.40604546,  288.19780359,  480.66569994,\n",
       "         326.19131414,  -57.72015204,  -92.12095315,  -39.21298269,\n",
       "         466.97495339,  292.1955603 ,  158.96024893, -116.80961873,\n",
       "         532.28461546, -308.73764418, -206.8802986 ,  411.19880519,\n",
       "         245.14751718,  500.07038317,  -82.2514126 ,  391.96650853,\n",
       "        -173.98487322, -464.79271492,  176.99004743,  332.05276553,\n",
       "         610.9809235 , -300.26171147,  343.52331234,  281.79826411,\n",
       "        -234.48293438,  -82.45240449,  310.11041844,  -32.90532179,\n",
       "        -148.88446996,  623.72848089,  262.29810794,  553.84633519,\n",
       "        -189.94337898, -199.71119273,  689.31597957, -137.42351807,\n",
       "        -140.97602182, -204.58044917, -175.87401579, -120.80997257,\n",
       "          64.13682105,   31.44523324,   12.47747506, -447.95350836,\n",
       "         538.81925672,   10.25599822, -243.87723061, -504.78734432,\n",
       "         347.69632795, -207.39447424,  105.87167174,  285.34986015,\n",
       "         -71.33752134,  248.93552661, -135.4310159 ,  668.42943062,\n",
       "        -165.35610327, -220.3405967 ,  251.22903124,  550.83466799,\n",
       "        -272.68375885,  687.53557989,    5.46101621, -229.61492743,\n",
       "         670.04673559,   97.1356315 ,  -42.46289672,  679.59713665,\n",
       "         -20.56146655,  254.37048636,  -35.1137173 ,  -60.03201763,\n",
       "        -210.94086803, -388.14876914, -289.25889305,  128.35162979,\n",
       "         324.307757  ,  123.15830796, -461.45054577, -549.24372652,\n",
       "        -205.42865326,  -52.02184686,  -66.72521227, -462.22547555,\n",
       "        -351.96766835,  -88.21951514,  -29.23283909,  -86.43128015,\n",
       "         537.42908807,  543.48158001,  317.0624121 , -271.75963596,\n",
       "        -126.50237466,   53.37551106,  574.40769602,  549.91020303,\n",
       "        -202.4461359 ,   38.58147495,  298.62386053,  -49.6201714 ,\n",
       "         -40.22598206, -339.3147478 ,  -92.51900105, -285.91984987,\n",
       "        -138.42854418, -292.01784293, -142.89960788,   60.99650182,\n",
       "         583.56077011,   64.00880326, -110.73789877,  542.76932231,\n",
       "         312.806162  , -330.47257535,   20.26925749,  384.63278631,\n",
       "         540.52104523, -111.69106383,  271.24205223, -223.58113743,\n",
       "        -265.08056392, -112.16806871, -149.90356615,  301.71062341,\n",
       "        -348.65876499, -121.70429258, -172.79786075,  269.69037775,\n",
       "        -243.73957984,  439.86403506,  -62.33869897,   24.38221245,\n",
       "           6.60183085,  222.2202388 ,  382.92026985, -199.28381402,\n",
       "         306.34121153,  211.82337227, -152.11513173]),\n",
       " 0.20000029081015863)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "lagr_mult=fit_svm(X_train,t_train,linear_kernel)\n",
    "# Fit SVM on training data\n",
    "extract_parameters(X_train, t_train, linear_kernel, lagr_mult,)\n",
    "\n",
    "# Extract parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2\n",
    "\n",
    "Having learnt an SVM, we can use the calculated parameters to make predictions on novel samples.\n",
    "- Implement a function `svm_predict(X, kernel, lagr_mult, svs, sv_labels, intercept)`.\n",
    "- Use this function with the linear kernel and compute the test accuracy on the 2d dataset.\n",
    "- Visualize the samples form the test set in a scatter plot colored by your predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def svm_predict(X, kernel, lagr_mult, svs, sv_labels, intercept):\n",
    "    ''' Given the learned parameters of the SVM, make a prediction on the test set.\n",
    "    Inputs\n",
    "        X:         predictors\n",
    "        kernel:    a kernel to be used\n",
    "        lagr_mult: the Lagrange multipliers obtained by solving the dual QP\n",
    "        svs:       set of support vectors\n",
    "        sv_labels: targets t_n for the support vectors\n",
    "        intercept: computed intercept (also called bias)\n",
    "    \n",
    "    Returns\n",
    "        prediction: predictions on novel samples\n",
    "    '''  \n",
    "    \n",
    "    # ---------------- INSERT CODE ----------------------\n",
    "\n",
    "\n",
    "\n",
    "    # ---------------- END CODE -------------------------\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "# make predictions for test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3\n",
    "\n",
    "- Instead of using the linear kernel, use the Gaussian RBF kernel defined in Practical 3.\n",
    "- Compare results on with both kernels with sklearn implementation (SVC)\n",
    "- Visualize the predictions on the test set, the learned support vectors and the decision boundary for both kernels (Hint: Adapt the decision boundary plot from Practical 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(a, b):\n",
    "    # ---------------- INSERT CODE ----------------------\n",
    "\n",
    "\n",
    "\n",
    "    # ---------------- END CODE -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit SVM with rbf kernel and calculate the test accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit SVM using sklearn and calculate the test accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Decision Trees\n",
    "\n",
    "Next, we will implement a simple decision tree classifier using the Wine dataset, one of the standard sklearn datasets. \n",
    "\n",
    "We will use the Gini impurity as a criterion for splitting. It is defined for a set of labels as\n",
    "$$ G = \\sum_{i=0}^C p(i) * (1- p(i)) $$\n",
    "\n",
    "Given labels $l$ and split $l_a$ and $l_b$, the weighted removed impurity can be computed by $G(l) - \\frac{|l_a|}{|l|}G(l_a) - \\frac{|l_b|}{|l|}G(l_b)$.\n",
    "\n",
    "Here is a simple explanation of the Gini impurity that you may find useful: https://victorzhou.com/blog/gini-impurity/\n",
    "\n",
    "\n",
    "### Task 2.1\n",
    "\n",
    "1. Plot the distribution of the first feature of for each class of the wine dataset.\n",
    "2. Implement a function `gini_impurity(t)` that computes the Gini impurity for an array of labels `t`.\n",
    "3. Calculate the removed Gini impurity for a split after 50 samples, i.e. between `t[:50]` and `t[50:]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Wine dataset and split into train+test set\n",
    "\n",
    "X, t = load_wine(return_X_y=True)\n",
    "X_train, X_test, t_train, t_test = split_data(X, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Gini impurity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2\n",
    "For each of the first 12 features, compute the remove Gini impurity for every possible split. Visualize the removed Gini impurity per feature across all splits. Which is the optimal split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3\n",
    "\n",
    "1. Implement a function `build_tree(X, t, depth)` which recursively builds a tree. Use the classes `Node` and `Leaf` as a data structure to build your tree.\n",
    "2. Implement a function `predict_tree(tree, x)` which makes a prediction for sample `x`. Obtain scores for the `wine` dataset and compare to `sklearn.tree.DecisionTree`.\n",
    "3. Switch back to the synthetic 2d dataset from the beginning (kernel methods). Compute scores and visualize the decisions in a 2d grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, left, right, n_feat, threshold):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.n_feat = n_feat\n",
    "        self.threshold = threshold\n",
    "\n",
    "\n",
    "class Leaf:\n",
    "    def __init__(self, label):\n",
    "        self.label = label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement recursive tree function\n",
    "\n",
    "def build_tree(X, t, depth, max_depth=3, n_labels=2):\n",
    "    \n",
    "    # ---------------- INSERT CODE ----------------------\n",
    "\n",
    "\n",
    "\n",
    "    # ---------------- END CODE -------------------------\n",
    "\n",
    "    \n",
    "def predict_tree(node, x):\n",
    "    \n",
    "    # ---------------- INSERT CODE ----------------------\n",
    "\n",
    "\n",
    "\n",
    "    # ---------------- END CODE -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build tree\n",
    "\n",
    "tree = build_tree(X_train, t_train, 0, max_depth=3, n_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate training and test scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate test score using sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate test score for synthetic 2D dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
